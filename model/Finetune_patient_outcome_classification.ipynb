{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch import optim\n",
    "from FinetunePatientClassification import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: bench kept crashing during the train and validation loop. According to torch profiler, the training loop ran but crashed somewhere during validation. There is a sudden spike in GPU usage then the error message appeared. \n",
    "Troubleshooting attempts:\n",
    "x tried downsampling data\n",
    "x batch size\n",
    "x add gradient accumulation\n",
    "x mixed precision training\n",
    "x freeze model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the preprocessed data for scFoundation\n",
    "data = pd.read_csv('gene_symbol_converted_data_for_scF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample the data due to GPU limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response_3m\n",
       "1    13160\n",
       "0    11906\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_csv('patient_metadata.csv')\n",
    "labels_df[\"Response_3m\"].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "patients = labels_df.patient_id.unique()\n",
    "\n",
    "sample_size = 3 # number of patients to sample\n",
    "\n",
    "# randomly sample patient IDs\n",
    "random_seed=42\n",
    "sampled_patient_ids = random.sample(list(patients), sample_size)\n",
    "\n",
    "# Create a new DF with only the sampled patients\n",
    "labels_df_downsampled = labels_df[labels_df['patient_id'].isin(sampled_patient_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample the expression data accordingly\n",
    "data_downsampled = data.iloc[labels_df_downsampled.index]\n",
    "\n",
    "# remove the cell_id column\n",
    "data_downsampled = data_downsampled.drop(columns=['cell_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1149, 19264), (1149, 7))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure the data and labels are aligned\n",
    "data_downsampled.shape, labels_df_downsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A3GALT2</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>A4GNT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AADAC</th>\n",
       "      <th>...</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11A</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17841</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17842</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17843</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17844</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17845</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1149 rows Ã— 19264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       A1BG  A1CF  A2M  A2ML1  A3GALT2  A4GALT  A4GNT  AAAS  AACS  AADAC  ...  \\\n",
       "414     0.0   0.0  0.0    0.0      0.0     0.0    0.0   0.0   0.0    0.0  ...   \n",
       "415     0.0   0.0  0.0    0.0      0.0     0.0    0.0   0.0   0.0    0.0  ...   \n",
       "416     0.0   0.0  0.0    0.0      0.0     0.0    0.0   0.0   0.0    0.0  ...   \n",
       "417     0.0   0.0  0.0    0.0      0.0     0.0    0.0   0.0   0.0    0.0  ...   \n",
       "418     0.0   0.0  0.0    0.0      0.0     0.0    0.0   0.0   1.0    0.0  ...   \n",
       "...     ...   ...  ...    ...      ...     ...    ...   ...   ...    ...  ...   \n",
       "17841   0.0   0.0  0.0    0.0      0.0     0.0    0.0   0.0   0.0    0.0  ...   \n",
       "17842   0.0   0.0  0.0    0.0      0.0     0.0    0.0   0.0   0.0    0.0  ...   \n",
       "17843   0.0   0.0  0.0    0.0      0.0     0.0    0.0   0.0   0.0    0.0  ...   \n",
       "17844   0.0   0.0  0.0    0.0      0.0     0.0    0.0   0.0   0.0    0.0  ...   \n",
       "17845   0.0   0.0  0.0    0.0      0.0     0.0    0.0   0.0   0.0    0.0  ...   \n",
       "\n",
       "       ZWILCH  ZWINT  ZXDA  ZXDB  ZXDC  ZYG11A  ZYG11B  ZYX  ZZEF1  ZZZ3  \n",
       "414       0.0    0.0   0.0   0.0   0.0     0.0     0.0  0.0    0.0   0.0  \n",
       "415       0.0    0.0   0.0   0.0   0.0     0.0     0.0  3.0    0.0   0.0  \n",
       "416       0.0    1.0   0.0   0.0   0.0     0.0     0.0  5.0    0.0   0.0  \n",
       "417       0.0    0.0   0.0   0.0   0.0     0.0     0.0  0.0    0.0   0.0  \n",
       "418       0.0    0.0   0.0   0.0   0.0     0.0     0.0  0.0    0.0   0.0  \n",
       "...       ...    ...   ...   ...   ...     ...     ...  ...    ...   ...  \n",
       "17841     0.0    0.0   0.0   0.0   0.0     0.0     0.0  2.0    0.0   0.0  \n",
       "17842     2.0    2.0   0.0   0.0   0.0     0.0     0.0  5.0    0.0   0.0  \n",
       "17843     1.0    1.0   0.0   0.0   0.0     0.0     0.0  3.0    0.0   0.0  \n",
       "17844     0.0    0.0   0.0   0.0   0.0     0.0     0.0  0.0    0.0   0.0  \n",
       "17845     0.0    0.0   0.0   0.0   0.0     0.0     0.0  0.0    0.0   0.0  \n",
       "\n",
       "[1149 rows x 19264 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleCellDataset(Dataset):\n",
    "    def __init__(self, gene_expression_csv, labels_df, label_encoder):\n",
    "        # Load the gene expression data\n",
    "        self.gene_expression = gene_expression_csv\n",
    "        \n",
    "        # Ensure the labels DataFrame has the same number of rows as the gene expression data\n",
    "        assert len(self.gene_expression) == len(labels_df), \"Mismatch in number of samples between gene expression data and labels\"\n",
    "        \n",
    "        # Convert labels to numeric if they're categorical\n",
    "        \n",
    "        self.labels = torch.LongTensor(label_encoder.transform(labels_df['Response_3m']))\n",
    "        \n",
    "        \n",
    "        # Convert gene expression data to torch tensor\n",
    "        self.gene_expression = torch.FloatTensor(self.gene_expression.values.astype(np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'x': self.gene_expression[idx],\n",
    "            'targets': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "def print_gpu_memory(step_name):\n",
    "    print(f\"GPU memory at {step_name}: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "def finetune_scFoundation(gene_expression_csv, labels_df, model_class, ckpt_path,\n",
    "                          batch_size=4, num_epochs=5, lr=0.001,\n",
    "                          validation_split=0.2, device='cuda',\n",
    "                          gradient_accumulation_steps=2):\n",
    "    \n",
    "    gene_exp_train, gene_exp_val, labels_train, labels_val = train_test_split(gene_expression_csv, \n",
    "                                                                              labels_df, test_size=validation_split, \n",
    "                                                                              random_state=42)\n",
    "\n",
    "    # Fit LabelEncoder on combined dataset\n",
    "    le = LabelEncoder()\n",
    "    combined_labels = pd.concat([labels_train['Response_3m'], labels_val['Response_3m']])\n",
    "    le.fit(combined_labels)\n",
    "\n",
    "    #create datasets\n",
    "    train_dataset = SingleCellDataset(gene_exp_train, labels_train, le)\n",
    "    val_dataset = SingleCellDataset(gene_exp_val, labels_val, le)\n",
    "\n",
    "    #create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = batch_size // 2 , shuffle=False, num_workers=4, pin_memory=True)  # Ensure at least 2, but no more than 8\n",
    "\n",
    "    # Initialize model\n",
    "    model = model_class(ckpt_path=ckpt_path)\n",
    "    model.build()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Initialize gradient scaler for mixed precision training\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Optionally load checkpoint\n",
    "    start_epoch = 0\n",
    "    \n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            profile_memory=True, record_shapes=True) as prof:\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "\n",
    "            # Clear CUDA cache\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            for i, batch in enumerate(train_loader):\n",
    "                print_gpu_memory(f\"Epoch {epoch}, Batch {i} start\")\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "                # Mixed precision training\n",
    "                with autocast():\n",
    "                    # Forward pass\n",
    "                    logits = model(batch)\n",
    "\n",
    "                    # Compute loss\n",
    "                    loss = model.compute_loss(logits, batch['targets'].float()) / gradient_accumulation_steps\n",
    "                \n",
    "                print_gpu_memory(f\"Epoch {epoch}, Batch {i} after forward pass\")\n",
    "                # Ensure loss requires gradient\n",
    "                assert loss.requires_grad, \"Loss does not require gradients\"\n",
    "\n",
    "                # Backward pass with gradient scaling\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                print_gpu_memory(f\"Epoch {epoch}, Batch {i} after backward pass\")\n",
    "\n",
    "                if (i + 1) % gradient_accumulation_steps == 0:\n",
    "                    # Unscale gradients and optimizer step\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # add garbage collection\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                    print_gpu_memory(f\"Epoch {epoch}, Batch {i} after optimizer step\")\n",
    "\n",
    "                train_loss += loss.item() * gradient_accumulation_steps\n",
    "            print(f\"Max GPU memory allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "    \n",
    "\n",
    "\n",
    "            #validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            print_gpu_memory(f\"Before validation\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for j, batch in enumerate(val_loader):\n",
    "                    \n",
    "                    print_gpu_memory(f\"Validation, Batch {j} start\")\n",
    "                    if batch['x'].size(0) < 2:  # Skip batches smaller than 2\n",
    "                        continue\n",
    "                    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                    logits = model(batch)\n",
    "                    val_loss += model.compute_loss(logits, batch['targets'].float()).item()\n",
    "                    predicted = (torch.sigmoid(logits) > 0.5).float()\n",
    "                    print(\"predicted: \", predicted)\n",
    "                    # calculate correct predictions\n",
    "                    correct_pred = (predicted == batch['targets']).sum().item()\n",
    "                    print(f'correct predictions: {correct_pred}')\n",
    "\n",
    "\n",
    "                    total += batch['targets'].size(0)\n",
    "                    correct += correct_pred\n",
    "\n",
    "                    # Move data back to CPU\n",
    "                    for k in batch.keys():\n",
    "                        batch[k] = batch[k].cpu()\n",
    "                    del batch, logits, predicted\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    print_gpu_memory(f\"Validation, Batch {j} end\")\n",
    "\n",
    "            \n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"total samples in validation: {total}; correct predictions: {correct}\")\n",
    "            print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "            print(f\"Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
    "            if total == 0:\n",
    "                print(\"No validation samples\")\n",
    "            else:\n",
    "                print(f\"Validation Accuracy: {100*correct/total:.2f}%\")\n",
    "            print(\"-----------------------------\")\n",
    "    \n",
    "    \n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "    print(f\"Peak CUDA memory allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"Peak CUDA memory reserved: {torch.cuda.max_memory_reserved() / 1e9:.2f} GB\")\n",
    "    print(\"\\nCUDA Memory Summary:\")\n",
    "    print(torch.cuda.memory_summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mask_gene_name': False, 'gene_num': 19266, 'seq_len': 19266, 'encoder': {'hidden_dim': 768, 'depth': 12, 'heads': 12, 'dim_head': 64, 'seq_len': 19266, 'module_type': 'transformer', 'norm_first': False}, 'decoder': {'hidden_dim': 512, 'depth': 6, 'heads': 8, 'dim_head': 64, 'module_type': 'performer', 'seq_len': 19266, 'norm_first': False}, 'n_class': 104, 'pad_token_id': 103, 'mask_token_id': 102, 'bin_num': 100, 'bin_alpha': 1.0, 'rawcount': True, 'model': 'mae_autobin', 'test_valid_train_idx_dict': '/nfs_beijing/minsheng/data/os10000w-new/global_shuffle/meta.csv.train_set_idx_dict.pt', 'valid_data_path': '/nfs_beijing/minsheng/data/valid_count_10w.npz', 'num_tokens': 13, 'train_data_path': None, 'isPanA': False, 'isPlanA1': False, 'max_files_to_load': 5, 'bin_type': 'auto_bin', 'value_mask_prob': 0.3, 'zero_mask_prob': 0.03, 'replace_prob': 0.8, 'random_token_prob': 0.1, 'mask_ignore_token_ids': [0], 'decoder_add_zero': True, 'mae_encoder_max_seq_len': 15000, 'isPlanA': False, 'mask_prob': 0.3, 'model_type': 'mae_autobin', 'pos_embed': False, 'device': 'cuda'}\n",
      "self.encoder.transformer_encoder  0.self_attn.in_proj_weight  no grad\n",
      "self.encoder.transformer_encoder  0.self_attn.in_proj_bias  no grad\n",
      "self.encoder.transformer_encoder  0.self_attn.out_proj.weight  no grad\n",
      "self.encoder.transformer_encoder  0.self_attn.out_proj.bias  no grad\n",
      "self.encoder.transformer_encoder  0.linear1.weight  no grad\n",
      "self.encoder.transformer_encoder  0.linear1.bias  no grad\n",
      "self.encoder.transformer_encoder  0.linear2.weight  no grad\n",
      "self.encoder.transformer_encoder  0.linear2.bias  no grad\n",
      "self.encoder.transformer_encoder  0.norm1.weight  no grad\n",
      "self.encoder.transformer_encoder  0.norm1.bias  no grad\n",
      "self.encoder.transformer_encoder  0.norm2.weight  no grad\n",
      "self.encoder.transformer_encoder  0.norm2.bias  no grad\n",
      "self.encoder.transformer_encoder  1.self_attn.in_proj_weight  no grad\n",
      "self.encoder.transformer_encoder  1.self_attn.in_proj_bias  no grad\n",
      "self.encoder.transformer_encoder  1.self_attn.out_proj.weight  no grad\n",
      "self.encoder.transformer_encoder  1.self_attn.out_proj.bias  no grad\n",
      "self.encoder.transformer_encoder  1.linear1.weight  no grad\n",
      "self.encoder.transformer_encoder  1.linear1.bias  no grad\n",
      "self.encoder.transformer_encoder  1.linear2.weight  no grad\n",
      "self.encoder.transformer_encoder  1.linear2.bias  no grad\n",
      "self.encoder.transformer_encoder  1.norm1.weight  no grad\n",
      "self.encoder.transformer_encoder  1.norm1.bias  no grad\n",
      "self.encoder.transformer_encoder  1.norm2.weight  no grad\n",
      "self.encoder.transformer_encoder  1.norm2.bias  no grad\n",
      "self.encoder.transformer_encoder  2.self_attn.in_proj_weight  no grad\n",
      "self.encoder.transformer_encoder  2.self_attn.in_proj_bias  no grad\n",
      "self.encoder.transformer_encoder  2.self_attn.out_proj.weight  no grad\n",
      "self.encoder.transformer_encoder  2.self_attn.out_proj.bias  no grad\n",
      "self.encoder.transformer_encoder  2.linear1.weight  no grad\n",
      "self.encoder.transformer_encoder  2.linear1.bias  no grad\n",
      "self.encoder.transformer_encoder  2.linear2.weight  no grad\n",
      "self.encoder.transformer_encoder  2.linear2.bias  no grad\n",
      "self.encoder.transformer_encoder  2.norm1.weight  no grad\n",
      "self.encoder.transformer_encoder  2.norm1.bias  no grad\n",
      "self.encoder.transformer_encoder  2.norm2.weight  no grad\n",
      "self.encoder.transformer_encoder  2.norm2.bias  no grad\n",
      "self.encoder.transformer_encoder  3.self_attn.in_proj_weight  no grad\n",
      "self.encoder.transformer_encoder  3.self_attn.in_proj_bias  no grad\n",
      "self.encoder.transformer_encoder  3.self_attn.out_proj.weight  no grad\n",
      "self.encoder.transformer_encoder  3.self_attn.out_proj.bias  no grad\n",
      "self.encoder.transformer_encoder  3.linear1.weight  no grad\n",
      "self.encoder.transformer_encoder  3.linear1.bias  no grad\n",
      "self.encoder.transformer_encoder  3.linear2.weight  no grad\n",
      "self.encoder.transformer_encoder  3.linear2.bias  no grad\n",
      "self.encoder.transformer_encoder  3.norm1.weight  no grad\n",
      "self.encoder.transformer_encoder  3.norm1.bias  no grad\n",
      "self.encoder.transformer_encoder  3.norm2.weight  no grad\n",
      "self.encoder.transformer_encoder  3.norm2.bias  no grad\n",
      "self.encoder.transformer_encoder  4.self_attn.in_proj_weight  no grad\n",
      "self.encoder.transformer_encoder  4.self_attn.in_proj_bias  no grad\n",
      "self.encoder.transformer_encoder  4.self_attn.out_proj.weight  no grad\n",
      "self.encoder.transformer_encoder  4.self_attn.out_proj.bias  no grad\n",
      "self.encoder.transformer_encoder  4.linear1.weight  no grad\n",
      "self.encoder.transformer_encoder  4.linear1.bias  no grad\n",
      "self.encoder.transformer_encoder  4.linear2.weight  no grad\n",
      "self.encoder.transformer_encoder  4.linear2.bias  no grad\n",
      "self.encoder.transformer_encoder  4.norm1.weight  no grad\n",
      "self.encoder.transformer_encoder  4.norm1.bias  no grad\n",
      "self.encoder.transformer_encoder  4.norm2.weight  no grad\n",
      "self.encoder.transformer_encoder  4.norm2.bias  no grad\n",
      "self.encoder.transformer_encoder  5.self_attn.in_proj_weight  no grad\n",
      "self.encoder.transformer_encoder  5.self_attn.in_proj_bias  no grad\n",
      "self.encoder.transformer_encoder  5.self_attn.out_proj.weight  no grad\n",
      "self.encoder.transformer_encoder  5.self_attn.out_proj.bias  no grad\n",
      "self.encoder.transformer_encoder  5.linear1.weight  no grad\n",
      "self.encoder.transformer_encoder  5.linear1.bias  no grad\n",
      "self.encoder.transformer_encoder  5.linear2.weight  no grad\n",
      "self.encoder.transformer_encoder  5.linear2.bias  no grad\n",
      "self.encoder.transformer_encoder  5.norm1.weight  no grad\n",
      "self.encoder.transformer_encoder  5.norm1.bias  no grad\n",
      "self.encoder.transformer_encoder  5.norm2.weight  no grad\n",
      "self.encoder.transformer_encoder  5.norm2.bias  no grad\n",
      "self.encoder.transformer_encoder  6.self_attn.in_proj_weight  no grad\n",
      "self.encoder.transformer_encoder  6.self_attn.in_proj_bias  no grad\n",
      "self.encoder.transformer_encoder  6.self_attn.out_proj.weight  no grad\n",
      "self.encoder.transformer_encoder  6.self_attn.out_proj.bias  no grad\n",
      "self.encoder.transformer_encoder  6.linear1.weight  no grad\n",
      "self.encoder.transformer_encoder  6.linear1.bias  no grad\n",
      "self.encoder.transformer_encoder  6.linear2.weight  no grad\n",
      "self.encoder.transformer_encoder  6.linear2.bias  no grad\n",
      "self.encoder.transformer_encoder  6.norm1.weight  no grad\n",
      "self.encoder.transformer_encoder  6.norm1.bias  no grad\n",
      "self.encoder.transformer_encoder  6.norm2.weight  no grad\n",
      "self.encoder.transformer_encoder  6.norm2.bias  no grad\n",
      "self.encoder.transformer_encoder  7.self_attn.in_proj_weight  no grad\n",
      "self.encoder.transformer_encoder  7.self_attn.in_proj_bias  no grad\n",
      "self.encoder.transformer_encoder  7.self_attn.out_proj.weight  no grad\n",
      "self.encoder.transformer_encoder  7.self_attn.out_proj.bias  no grad\n",
      "self.encoder.transformer_encoder  7.linear1.weight  no grad\n",
      "self.encoder.transformer_encoder  7.linear1.bias  no grad\n",
      "self.encoder.transformer_encoder  7.linear2.weight  no grad\n",
      "self.encoder.transformer_encoder  7.linear2.bias  no grad\n",
      "self.encoder.transformer_encoder  7.norm1.weight  no grad\n",
      "self.encoder.transformer_encoder  7.norm1.bias  no grad\n",
      "self.encoder.transformer_encoder  7.norm2.weight  no grad\n",
      "self.encoder.transformer_encoder  7.norm2.bias  no grad\n",
      "self.encoder.transformer_encoder  8.self_attn.in_proj_weight  no grad\n",
      "self.encoder.transformer_encoder  8.self_attn.in_proj_bias  no grad\n",
      "self.encoder.transformer_encoder  8.self_attn.out_proj.weight  no grad\n",
      "self.encoder.transformer_encoder  8.self_attn.out_proj.bias  no grad\n",
      "self.encoder.transformer_encoder  8.linear1.weight  no grad\n",
      "self.encoder.transformer_encoder  8.linear1.bias  no grad\n",
      "self.encoder.transformer_encoder  8.linear2.weight  no grad\n",
      "self.encoder.transformer_encoder  8.linear2.bias  no grad\n",
      "self.encoder.transformer_encoder  8.norm1.weight  no grad\n",
      "self.encoder.transformer_encoder  8.norm1.bias  no grad\n",
      "self.encoder.transformer_encoder  8.norm2.weight  no grad\n",
      "self.encoder.transformer_encoder  8.norm2.bias  no grad\n",
      "self.encoder.transformer_encoder  9.self_attn.in_proj_weight  no grad\n",
      "self.encoder.transformer_encoder  9.self_attn.in_proj_bias  no grad\n",
      "self.encoder.transformer_encoder  9.self_attn.out_proj.weight  no grad\n",
      "self.encoder.transformer_encoder  9.self_attn.out_proj.bias  no grad\n",
      "self.encoder.transformer_encoder  9.linear1.weight  no grad\n",
      "self.encoder.transformer_encoder  9.linear1.bias  no grad\n",
      "self.encoder.transformer_encoder  9.linear2.weight  no grad\n",
      "self.encoder.transformer_encoder  9.linear2.bias  no grad\n",
      "self.encoder.transformer_encoder  9.norm1.weight  no grad\n",
      "self.encoder.transformer_encoder  9.norm1.bias  no grad\n",
      "self.encoder.transformer_encoder  9.norm2.weight  no grad\n",
      "self.encoder.transformer_encoder  9.norm2.bias  no grad\n",
      "self.encoder.transformer_encoder  10.self_attn.in_proj_weight  no grad\n",
      "self.encoder.transformer_encoder  10.self_attn.in_proj_bias  no grad\n",
      "self.encoder.transformer_encoder  10.self_attn.out_proj.weight  no grad\n",
      "self.encoder.transformer_encoder  10.self_attn.out_proj.bias  no grad\n",
      "self.encoder.transformer_encoder  10.linear1.weight  no grad\n",
      "self.encoder.transformer_encoder  10.linear1.bias  no grad\n",
      "self.encoder.transformer_encoder  10.linear2.weight  no grad\n",
      "self.encoder.transformer_encoder  10.linear2.bias  no grad\n",
      "self.encoder.transformer_encoder  10.norm1.weight  no grad\n",
      "self.encoder.transformer_encoder  10.norm1.bias  no grad\n",
      "self.encoder.transformer_encoder  10.norm2.weight  no grad\n",
      "self.encoder.transformer_encoder  10.norm2.bias  no grad\n",
      "self.encoder.transformer_encoder  11.self_attn.in_proj_weight  no grad\n",
      "self.encoder.transformer_encoder  11.self_attn.in_proj_bias  no grad\n",
      "self.encoder.transformer_encoder  11.self_attn.out_proj.weight  no grad\n",
      "self.encoder.transformer_encoder  11.self_attn.out_proj.bias  no grad\n",
      "self.encoder.transformer_encoder  11.linear1.weight  no grad\n",
      "self.encoder.transformer_encoder  11.linear1.bias  no grad\n",
      "self.encoder.transformer_encoder  11.linear2.weight  no grad\n",
      "self.encoder.transformer_encoder  11.linear2.bias  no grad\n",
      "self.encoder.transformer_encoder  11.norm1.weight  no grad\n",
      "self.encoder.transformer_encoder  11.norm1.bias  no grad\n",
      "self.encoder.transformer_encoder  11.norm2.weight  no grad\n",
      "self.encoder.transformer_encoder  11.norm2.bias  no grad\n",
      "self.fc1  0.weight  have grad\n",
      "self.fc1  0.bias  have grad\n",
      "self.fc1  2.weight  have grad\n",
      "self.fc1  2.bias  have grad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-07-26 17:55:44 120286:120286 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory at Epoch 0, Batch 0 start: 0.40 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 0 after forward pass: 0.41 GB\n",
      "GPU memory at Epoch 0, Batch 0 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 1 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 1 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 1 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 1 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 2 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 2 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 2 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 3 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 3 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 3 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 3 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 4 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 4 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 4 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 5 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 5 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 5 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 5 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 6 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 6 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 6 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 7 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 7 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 7 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 7 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 8 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 8 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 8 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 9 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 9 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 9 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 9 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 10 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 10 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 10 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 11 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 11 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 11 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 11 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 12 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 12 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 12 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 13 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 13 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 13 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 13 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 14 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 14 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 14 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 15 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 15 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 15 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 15 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 16 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 16 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 16 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 17 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 17 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 17 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 17 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 18 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 18 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 18 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 19 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 19 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 19 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 19 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 20 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 20 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 20 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 21 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 21 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 21 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 21 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 22 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 22 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 22 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 23 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 23 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 23 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 23 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 24 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 24 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 24 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 25 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 25 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 25 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 25 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 26 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 26 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 26 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 27 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 27 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 27 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 27 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 28 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 28 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 28 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 29 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 29 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 29 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 29 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 30 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 30 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 30 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 31 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 31 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 31 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 31 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 32 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 32 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 32 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 33 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 33 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 33 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 33 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 34 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 34 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 34 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 35 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 35 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 35 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 35 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 36 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 36 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 36 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 37 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 37 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 37 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 37 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 38 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 38 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 38 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 39 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 39 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 39 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 39 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 40 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 40 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 40 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 41 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 41 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 41 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 41 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 42 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 42 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 42 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 43 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 43 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 43 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 43 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 44 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 44 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 44 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 45 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 45 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 45 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 45 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 46 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 46 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 46 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 47 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 47 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 47 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 47 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 48 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 48 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 48 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 49 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 49 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 49 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 49 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 50 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 50 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 50 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 51 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 51 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 51 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 51 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 52 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 52 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 52 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 53 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 53 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 53 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 53 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 54 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 54 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 54 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 55 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 55 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 55 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 55 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 56 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 56 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 56 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 57 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 57 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 57 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 57 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 58 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 58 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 58 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 59 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 59 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 59 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 59 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 60 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 60 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 60 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 61 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 61 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 61 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 61 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 62 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 62 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 62 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 63 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 63 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 63 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 63 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 64 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 64 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 64 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 65 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 65 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 65 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 65 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 66 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 66 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 66 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 67 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 67 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 67 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 67 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 68 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 68 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 68 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 69 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 69 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 69 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 69 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 70 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 70 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 70 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 71 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 71 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 71 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 71 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 72 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 72 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 72 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 73 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 73 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 73 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 73 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 74 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 74 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 74 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 75 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 75 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 75 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 75 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 76 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 76 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 76 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 77 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 77 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 77 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 77 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 78 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 78 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 78 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 79 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 79 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 79 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 79 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 80 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 80 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 80 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 81 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 81 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 81 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 81 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 82 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 82 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 82 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 83 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 83 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 83 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 83 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 84 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 84 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 84 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 85 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 85 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 85 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 85 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 86 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 86 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 86 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 87 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 87 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 87 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 87 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 88 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 88 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 88 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 89 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 89 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 89 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 89 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 90 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 90 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 90 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 91 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 91 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 91 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 91 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 92 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 92 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 92 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 93 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 93 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 93 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 93 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 94 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 94 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 94 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 95 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 95 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 95 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 95 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 96 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 96 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 96 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 97 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 97 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 97 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 97 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 98 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 98 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 98 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 99 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 99 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 99 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 99 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 100 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 100 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 100 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 101 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 101 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 101 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 101 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 102 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 102 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 102 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 103 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 103 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 103 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 103 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 104 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 104 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 104 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 105 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 105 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 105 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 105 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 106 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 106 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 106 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 107 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 107 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 107 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 107 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 108 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 108 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 108 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 109 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 109 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 109 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 109 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 110 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 110 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 110 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 111 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 111 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 111 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 111 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 112 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 112 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 112 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 113 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 113 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 113 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 113 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 114 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 114 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 114 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 115 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 115 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 115 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 115 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 116 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 116 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 116 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 117 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 117 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 117 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 117 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 118 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 118 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 118 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 119 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 119 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 119 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 119 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 120 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 120 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 120 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 121 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 121 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 121 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 121 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 122 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 122 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 122 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 123 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 123 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 123 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 123 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 124 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 124 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 124 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 125 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 125 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 125 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 125 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 126 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 126 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 126 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 127 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 127 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 127 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 127 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 128 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 128 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 128 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 129 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 129 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 129 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 129 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 130 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 130 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 130 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 131 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 131 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 131 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 131 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 132 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 132 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 132 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 133 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 133 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 133 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 133 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 134 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 134 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 134 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 135 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 135 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 135 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 135 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 136 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 136 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 136 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 137 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 137 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 137 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 137 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 138 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 138 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 138 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 139 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 139 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 139 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 139 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 140 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 140 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 140 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 141 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 141 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 141 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 141 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 142 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 142 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 142 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 143 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 143 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 143 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 143 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 144 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 144 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 144 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 145 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 145 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 145 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 145 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 146 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 146 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 146 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 147 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 147 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 147 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 147 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 148 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 148 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 148 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 149 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 149 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 149 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 149 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 150 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 150 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 150 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 151 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 151 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 151 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 151 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 152 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 152 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 152 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 153 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 153 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 153 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 153 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 154 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 154 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 154 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 155 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 155 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 155 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 155 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 156 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 156 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 156 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 157 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 157 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 157 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 157 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 158 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 158 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 158 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 159 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 159 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 159 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 159 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 160 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 160 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 160 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 161 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 161 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 161 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 161 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 162 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 162 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 162 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 163 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 163 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 163 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 163 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 164 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 164 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 164 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 165 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 165 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 165 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 165 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 166 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 166 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 166 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 167 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 167 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 167 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 167 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 168 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 168 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 168 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 169 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 169 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 169 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 169 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 170 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 170 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 170 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 171 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 171 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 171 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 171 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 172 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 172 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 172 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 173 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 173 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 173 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 173 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 174 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 174 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 174 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 175 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 175 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 175 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 175 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 176 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 176 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 176 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 177 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 177 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 177 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 177 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 178 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 178 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 178 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 179 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 179 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 179 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 179 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 180 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 180 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 180 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 181 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 181 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 181 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 181 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 182 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 182 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 182 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 183 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 183 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 183 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 183 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 184 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 184 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 184 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 185 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 185 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 185 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 185 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 186 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 186 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 186 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 187 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 187 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 187 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 187 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 188 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 188 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 188 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 189 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 189 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 189 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 189 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 190 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 190 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 190 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 191 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 191 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 191 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 191 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 192 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 192 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 192 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 193 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 193 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 193 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 193 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 194 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 194 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 194 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 195 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 195 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 195 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 195 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 196 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 196 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 196 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 197 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 197 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 197 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 197 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 198 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 198 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 198 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 199 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 199 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 199 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 199 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 200 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 200 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 200 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 201 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 201 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 201 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 201 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 202 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 202 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 202 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 203 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 203 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 203 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 203 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 204 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 204 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 204 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 205 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 205 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 205 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 205 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 206 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 206 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 206 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 207 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 207 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 207 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 207 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 208 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 208 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 208 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 209 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 209 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 209 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 209 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 210 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 210 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 210 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 211 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 211 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 211 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 211 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 212 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 212 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 212 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 213 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 213 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 213 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 213 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 214 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 214 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 214 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 215 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 215 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 215 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 215 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 216 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 216 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 216 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 217 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 217 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 217 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 217 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 218 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 218 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 218 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 219 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 219 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 219 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 219 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 220 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 220 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 220 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 221 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 221 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 221 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 221 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 222 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 222 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 222 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 223 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 223 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 223 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 223 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 224 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 224 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 224 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 225 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 225 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 225 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 225 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 226 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 226 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 226 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 227 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 227 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 227 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 227 after optimizer step: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 228 start: 0.42 GB\n",
      "logits shape:  torch.Size([4, 1])\n",
      "target shape:  torch.Size([4])\n",
      "squeezed logits shape:  torch.Size([4])\n",
      "GPU memory at Epoch 0, Batch 228 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 228 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 229 start: 0.42 GB\n",
      "logits shape:  torch.Size([3, 1])\n",
      "target shape:  torch.Size([3])\n",
      "squeezed logits shape:  torch.Size([3])\n",
      "GPU memory at Epoch 0, Batch 229 after forward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 229 after backward pass: 0.42 GB\n",
      "GPU memory at Epoch 0, Batch 229 after optimizer step: 0.42 GB\n",
      "Max GPU memory allocated: 1.00 GB\n",
      "GPU memory at Before validation: 0.42 GB\n",
      "GPU memory at Validation, Batch 0 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 0 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 1 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 1 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 2 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 2 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 3 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 3 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 4 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 4 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 5 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 5 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 6 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 6 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 7 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 7 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 8 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 8 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 9 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 9 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 10 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 10 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 11 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 11 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 12 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 12 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 13 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 13 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 14 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 14 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 15 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 15 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 16 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 16 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 17 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 17 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 18 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 18 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 19 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 19 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 20 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 20 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 21 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 21 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 22 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 22 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 23 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 23 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 24 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 24 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 25 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 25 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 26 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 26 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 27 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 27 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 28 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 28 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 29 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 29 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 30 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 30 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 31 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 31 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 32 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 32 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 33 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 33 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 34 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 34 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 35 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 35 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 36 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 36 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 37 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 37 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 38 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 38 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 39 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 0\n",
      "GPU memory at Validation, Batch 39 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 40 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 40 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 41 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 41 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 42 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 42 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 43 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 43 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 44 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 44 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 45 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 45 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 46 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 46 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 47 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 47 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 48 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 48 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 49 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 49 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 50 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 50 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 51 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 51 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 52 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 52 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 53 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 53 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 54 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 54 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 55 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 55 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 56 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 56 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 57 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 57 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 58 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 58 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 59 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 59 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 60 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 60 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 61 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 61 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 62 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 62 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 63 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 63 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 64 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 64 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 65 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 65 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 66 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 66 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 67 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 67 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 68 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 68 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 69 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 69 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 70 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 70 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 71 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 71 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 72 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 72 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 73 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 73 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 74 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 74 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 75 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 75 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 76 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 76 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 77 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 77 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 78 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 78 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 79 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 79 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 80 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 80 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 81 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 0\n",
      "GPU memory at Validation, Batch 81 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 82 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 82 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 83 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 0\n",
      "GPU memory at Validation, Batch 83 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 84 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 84 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 85 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 85 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 86 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 86 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 87 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 87 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 88 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 88 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 89 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 89 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 90 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 90 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 91 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 91 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 92 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 92 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 93 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 93 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 94 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 94 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 95 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 95 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 96 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 96 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 97 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 97 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 98 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 98 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 99 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 99 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 100 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 100 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 101 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 101 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 102 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 102 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 103 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 103 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 104 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 104 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 105 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 105 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 106 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 106 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 107 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 107 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 108 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 108 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 109 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 109 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 110 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [1.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 110 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 111 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 4\n",
      "GPU memory at Validation, Batch 111 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 112 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 112 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 113 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 113 end: 0.42 GB\n",
      "GPU memory at Validation, Batch 114 start: 0.42 GB\n",
      "logits shape:  torch.Size([2, 1])\n",
      "target shape:  torch.Size([2])\n",
      "squeezed logits shape:  torch.Size([2])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "correct predictions: 2\n",
      "GPU memory at Validation, Batch 114 end: 0.42 GB\n",
      "Epoch 1/1\n",
      "total samples in validation: 230; correct predictions: 312\n",
      "Train Loss: 0.3312\n",
      "Validation Loss: 0.6450\n",
      "Validation Accuracy: 135.65%\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-07-26 17:56:57 120286:120286 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-07-26 17:56:58 120286:120286 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::linear         0.05%      30.368ms         5.02%        2.799s     114.794us       0.000us         0.00%       45.189s       1.854ms           0 b           0 b     684.79 Gb    -100.88 Gb         24380  \n",
      "                                              aten::bmm         0.25%     139.375ms         0.62%     348.353ms      63.107us       19.276s        38.11%       19.368s       3.509ms           0 b           0 b     661.48 Gb     625.34 Gb          5520  \n",
      "                   aten::_transformer_encoder_layer_fwd         0.08%      41.960ms         3.20%        1.784s       1.293ms       0.000us         0.00%       16.338s      11.839ms           0 b           0 b      15.52 Gb    -103.54 Gb          1380  \n",
      "                                           aten::matmul         0.04%      24.107ms         0.71%     396.499ms     118.890us       0.000us         0.00%       15.979s       4.791ms           0 b           0 b     121.04 Gb    -500.29 Mb          3335  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us       15.728s        31.10%       15.728s       5.260ms           0 b           0 b           0 b           0 b          2990  \n",
      "                     aten::scaled_dot_product_attention         0.07%      41.520ms         1.43%     799.312ms     151.042us       0.000us         0.00%       15.455s       2.920ms         360 b     -43.12 Kb      73.27 Gb    -911.55 Mb          5292  \n",
      "                     aten::_native_multi_head_attention         0.17%      92.952ms         2.63%        1.464s       1.061ms       0.000us         0.00%       12.827s       9.295ms           0 b           0 b      15.75 Gb   -1198.74 Gb          1380  \n",
      "          aten::_scaled_dot_product_efficient_attention         0.04%      21.706ms         0.33%     186.493ms      67.570us       0.000us         0.00%        7.905s       2.864ms      43.12 Kb           0 b      38.04 Gb           0 b          2760  \n",
      "                     aten::_efficient_attention_forward         0.14%      77.867ms         0.23%     127.070ms      46.040us        7.738s        15.30%        7.870s       2.851ms      43.12 Kb       5.77 Kb      38.04 Gb           0 b          2760  \n",
      "fmha_cutlassF_f16_aligned_64x64_rf_sm80(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us        7.738s        15.30%        7.738s       2.804ms           0 b           0 b           0 b           0 b          2760  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 55.743s\n",
      "Self CUDA time total: 50.578s\n",
      "\n",
      "Peak CUDA memory allocated: 10.22 GB\n",
      "Peak CUDA memory reserved: 12.75 GB\n",
      "\n",
      "CUDA Memory Summary:\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 409357 KiB |   9745 MiB |   3953 GiB |   3953 GiB |\n",
      "|       from large pool | 406217 KiB |   9742 MiB |   3935 GiB |   3935 GiB |\n",
      "|       from small pool |   3140 KiB |     25 MiB |     18 GiB |     18 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 409357 KiB |   9745 MiB |   3953 GiB |   3953 GiB |\n",
      "|       from large pool | 406217 KiB |   9742 MiB |   3935 GiB |   3935 GiB |\n",
      "|       from small pool |   3140 KiB |     25 MiB |     18 GiB |     18 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 409353 KiB |   9742 MiB |   3943 GiB |   3943 GiB |\n",
      "|       from large pool | 406217 KiB |   9738 MiB |   3925 GiB |   3925 GiB |\n",
      "|       from small pool |   3136 KiB |     25 MiB |     18 GiB |     18 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 454656 KiB |  12160 MiB | 318592 MiB | 318148 MiB |\n",
      "|       from large pool | 448512 KiB |  12154 MiB | 318258 MiB | 317820 MiB |\n",
      "|       from small pool |   6144 KiB |     26 MiB |    334 MiB |    328 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  45298 KiB |   2306 MiB |   2052 GiB |   2052 GiB |\n",
      "|       from large pool |  42295 KiB |   2304 MiB |   2032 GiB |   2032 GiB |\n",
      "|       from small pool |   3003 KiB |      4 MiB |     19 GiB |     19 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     174    |     265    |  186029    |  185855    |\n",
      "|       from large pool |      51    |      62    |   87018    |   86967    |\n",
      "|       from small pool |     123    |     203    |   99011    |   98888    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     174    |     265    |  186029    |  185855    |\n",
      "|       from large pool |      51    |      62    |   87018    |   86967    |\n",
      "|       from small pool |     123    |     203    |   99011    |   98888    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      23    |      38    |    2258    |    2235    |\n",
      "|       from large pool |      20    |      33    |    2091    |    2071    |\n",
      "|       from small pool |       3    |      13    |     167    |     164    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      29    |      42    |   91480    |   91451    |\n",
      "|       from large pool |      20    |      29    |   57448    |   57428    |\n",
      "|       from small pool |       9    |      16    |   34032    |   34023    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finetuned_model = finetune_scFoundation(data_downsampled, labels_df_downsampled, model_class=FinetunePatientClassification, \n",
    "                                        ckpt_path='./models/models.ckpt', num_epochs=1, lr=0.001, device='cuda',\n",
    "                                        validation_split=0.2, batch_size=4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_finetuned_model(model, save_path, model_name):\n",
    "    \"\"\"\n",
    "    Save the finetuned model using both methods: entire model and state dict.\n",
    "    \n",
    "    Args:\n",
    "    model (torch.nn.Module): The finetuned model to save\n",
    "    save_path (str): Directory to save the model\n",
    "    model_name (str): Name to use for the saved model files\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # 1. Save the entire model\n",
    "    entire_model_path = os.path.join(save_path, f\"{model_name}_entire.pth\")\n",
    "    torch.save(model, entire_model_path)\n",
    "    print(f\"Entire model saved to {entire_model_path}\")\n",
    "    \n",
    "    # 2. Save only the state dict\n",
    "    state_dict_path = os.path.join(save_path, f\"{model_name}_state_dict.pth\")\n",
    "    torch.save(model.state_dict(), state_dict_path)\n",
    "    print(f\"Model state dict saved to {state_dict_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './saved_models'\n",
    "model_name = 'finetuned_scFoundation'\n",
    "save_finetuned_model(finetuned_model, save_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4])\n",
    "torch.unsqueeze(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Save the entire model\n",
    "entire_model_path = os.path.join(save_path, f\"{model_name}_entire.pth\")\n",
    "torch.save(model, entire_model_path)\n",
    "print(f\"Entire model saved to {entire_model_path}\")\n",
    "\n",
    "# 2. Save only the state dict\n",
    "state_dict_path = os.path.join(save_path, f\"{model_name}_state_dict.pth\")\n",
    "torch.save(model.state_dict(), state_dict_path)\n",
    "print(f\"Model state dict saved to {state_dict_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scFoundation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
